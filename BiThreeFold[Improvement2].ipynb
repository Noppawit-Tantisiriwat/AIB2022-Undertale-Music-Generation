{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicData(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, pkl_path, sequence_length, use_embedding=False, subset=None):\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.use_embedding = use_embedding\n",
    "\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "\n",
    "            self.notes = pickle.load(f)\n",
    "        if subset:\n",
    "\n",
    "            self.notes = self.notes[:subset]\n",
    "        \n",
    "        \n",
    "        self.n = len(set(self.notes))\n",
    "        self.pitchnames = sorted(set(item for item in self.notes))\n",
    "        self.note_to_int = dict((note, number) for number, note in enumerate(self.pitchnames))\n",
    "\n",
    "        data = []\n",
    "        label = []\n",
    "\n",
    "\n",
    "        for i in range(0, len(self.notes) - sequence_length, 1):\n",
    "            sequence_in = self.notes[i:i + sequence_length]\n",
    "            sequence_out = self.notes[i + sequence_length]\n",
    "            data.append([self.note_to_int[char] for char in sequence_in])\n",
    "            label.append(self.note_to_int[sequence_out])\n",
    "\n",
    "        n_patterns = len(data)\n",
    "\n",
    "\n",
    "        self.data = torch.tensor(np.reshape(data, (n_patterns, sequence_length, 1)))\n",
    "        #self.label = torch.tensor(label, dtype=torch.double)\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        x, y = self.data[index], self.label[index]\n",
    "\n",
    "        if self.use_embedding:\n",
    "            \n",
    "            return x, y\n",
    "\n",
    "        else:\n",
    "\n",
    "            x = x / float(self.n)\n",
    "\n",
    "            return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds  = MusicData(\"../data/notes_final.pkl\", sequence_length=100, use_embedding=True, subset=None) # full_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_val_ds = train_test_split(all_ds,test_size=0.2) # apply train_test_split from sklearn to \"all_ds\"\n",
    "val_ds, test_ds = train_test_split(test_val_ds,test_size=0.5) # apply train_test_split from sklearn to \"test_val_ds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "valloader = DataLoader(val_ds,batch_size=128,shuffle=False)\n",
    "testloader = DataLoader(test_ds,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicEmbeddingBaseline(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, hidden_size, post_embedding, vocab_size, bidirectional):\n",
    "\n",
    "        super(MusicEmbeddingBaseline, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.post_embedding = post_embedding\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.hidden_lstm_input = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(all_ds.n, self.embedding_size)\n",
    "\n",
    "        self.base_lstm = nn.LSTM(embedding_size, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
    "        self.hidden_lstm = nn.LSTM(self.hidden_lstm_input, self.hidden_lstm_input, batch_first=True)\n",
    "        self.output_lstm = nn.LSTM(self.hidden_lstm_input, self.post_embedding, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.post_embedding, self.vocab_size)\n",
    "\n",
    "        #self.classifier_activation = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "        \n",
    "    def init_hidden(self, size, bidirectional):\n",
    "        return (torch.zeros(1+ (1*int(bidirectional)), self.batch, size).to(device), torch.zeros(1+ (1*int(bidirectional)), self.batch, size).to(device))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.batch = x.shape[0]\n",
    "\n",
    "        h0, c0 = self.init_hidden(self.hidden_size, bidirectional=True)\n",
    "        h1, c1 = self.init_hidden(self.hidden_lstm_input, bidirectional=False)\n",
    "        h2, c2 = self.init_hidden(self.post_embedding, bidirectional=False)\n",
    "\n",
    "        x = self.embedding_layer(x)\n",
    "        #if len(x.shape) != 2:\n",
    "        x = torch.squeeze(x, dim=-2)\n",
    "    \n",
    "\n",
    "        o1, (h0, c0) = self.base_lstm(x, (h0, c0))\n",
    "        o2, (h1, c1) = self.hidden_lstm(o1, (h1, c1))\n",
    "        o3, (h2, c2) = self.output_lstm(o2, (h2, c2))\n",
    "\n",
    "        x = self.classifier(h2.view(self.batch, self.post_embedding))\n",
    "        #x = self.classifier_activation(x)\n",
    "\n",
    "\n",
    "        return x, (h0, h1, h2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicEmbeddingBaseline(embedding_size=50, hidden_size=512, post_embedding=128, vocab_size=all_ds.n, bidirectional=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = []\n",
    "running_valid_loss = []\n",
    "for i in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # epoch loss\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    ## Training with loss logging\n",
    "    pbar = tqdm(trainloader)\n",
    "    pbar.set_description(f\"Epoch - {i + 1} / {epochs}\")\n",
    "    for data, label in pbar:\n",
    "\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred, hidden = model(data)\n",
    "\n",
    "        loss = loss_function(pred, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * data.size(0)\n",
    "        pbar.set_postfix({\"loss\" : loss.item()})\n",
    "    \n",
    "\n",
    "    print(f\"Average CrossEntropyLoss of Epoch {i + 1} : {epoch_loss / len(trainloader.dataset)}\")\n",
    "    running_loss.append(epoch_loss / len(trainloader.dataset))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = 0\n",
    "\n",
    "    for data, label in valloader:\n",
    "\n",
    "        data, label = data.to(device), label.to(device)\n",
    "\n",
    "        pred, hidden = model(data)\n",
    "\n",
    "        loss = loss_function(pred, label)\n",
    "\n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "    print(f\"Validation of Epoch {i + 1} : {epoch_loss / len(valloader.dataset)}\")\n",
    "    running_valid_loss.append(epoch_loss / len(valloader.dataset))\n",
    "\n",
    "\n",
    "print(\"Finish training !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict() ,\"embedded_model_fulldata_50_epochs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(np.arange(epochs - 2), running_loss)\n",
    "plt.plot(np.arange(epochs - 2),  running_valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Reload pre-trained Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = MusicEmbeddingBaseline(embedding_size=50, hidden_size=512, post_embedding=128, vocab_size=1140, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded.load_state_dict(torch.load(\"embedded_model_fulldata_50_epochs.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusicEmbeddingBaseline(\n",
       "  (embedding_layer): Embedding(1140, 50)\n",
       "  (base_lstm): LSTM(50, 512, batch_first=True, bidirectional=True)\n",
       "  (hidden_lstm): LSTM(1024, 1024, batch_first=True)\n",
       "  (output_lstm): LSTM(1024, 128, batch_first=True)\n",
       "  (classifier): Linear(in_features=128, out_features=1140, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### utils function that writes .midi files\n",
    "\n",
    "def create_midi(prediction_output):\n",
    "    from music21 import note, chord, instrument, stream\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "        # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "            # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "            # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.25\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write(\"midi\", fp=\"test_midi_fulldata_random_02.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model = loaded.eval().to(device)\n",
    "\n",
    "start = np.random.randint(0, len(all_ds.data)-1)\n",
    "\n",
    "\n",
    "pitchnames = sorted(set(item for item in all_ds.notes))\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "pattern = all_ds.data[start]\n",
    "window_size = all_ds.sequence_length\n",
    "prediction_output = []\n",
    "\n",
    "# doing auto regressive generation\n",
    "for note_indx in tqdm(range(500)):\n",
    "    prediction_input = pattern.view(1, len(pattern), 1).to(device)[:, note_indx:note_indx+window_size, :]\n",
    "\n",
    "    prediction, _ = testing_model(prediction_input)\n",
    "    # get prediction\n",
    "    index = torch.argmax(prediction, dim=-1)\n",
    "    \n",
    "    # get note from prediction\n",
    "    result = int_to_note[index.item()]\n",
    "    # add to output\n",
    "    prediction_output.append(result)\n",
    "    # add predicted note to the input sequence\n",
    "    pattern = torch.cat((pattern.cpu(), torch.unsqueeze(index.cpu(), dim=0)), dim=0)\n",
    "\n",
    "    \n",
    "\n",
    "create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "testing_model = loaded.eval().to(device)\n",
    "\n",
    "start = np.random.randint(0, len(all_ds.data)-1)\n",
    "\n",
    "\n",
    "pitchnames = sorted(set(item for item in all_ds.notes))\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "pattern = torch.tensor(np.reshape(random.choices(list(range(len(int_to_note))), k=100), (100, 1)))\n",
    "window_size = all_ds.sequence_length\n",
    "prediction_output = []\n",
    "\n",
    "# doing auto regressive generation\n",
    "for note_indx in tqdm(range(200)):\n",
    "    prediction_input = pattern.view(1, len(pattern), 1).to(device)[:, note_indx:note_indx+window_size, :]\n",
    "\n",
    "    prediction, _ = testing_model(prediction_input)\n",
    "    # get prediction\n",
    "    index = torch.argmax(prediction, dim=-1)\n",
    "    \n",
    "    # get note from prediction\n",
    "    result = int_to_note[index.item()]\n",
    "    # add to output\n",
    "    prediction_output.append(result)\n",
    "    # add predicted note to the input sequence\n",
    "    pattern = torch.cat((pattern.cpu(), torch.unsqueeze(index.cpu(), dim=0)), dim=0)\n",
    "\n",
    "    \n",
    "\n",
    "create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14958d3aee5f1cad06795f787e54b96185c25fb40dfec723a5be941f3a531b8c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
